Kubernetes Fundamentals
6th September 2018

https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/
https://kubernetes.io/docs/setup/independent/high-availability/

Master Nodes:
> API Server
> Scheduler
> Controller Manager
> Kubelet
> Docker

Worker Nodes:
> Kubelet
> kube-proxy
> Docker

On the Master Node:

# systemctl status kubelet

# more /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
...
Environment="KUBELET_SYSTEM_PODS_ARGS=--pod-manifest-path=/etc/kubernetes/manifests --allow-privileged=true"
...

# cd /etc/kubernetes/manifests
# ls
etcd.yaml
kube-apiserver.yaml
kube-controller-manager.yaml
kube-scheduler.yaml

# kubectl get pods --all-namespaces

# docker images

# docker ps


On the Worker Node:

# system status kubelet

# more /etc/systemd/system/kubelet.service.d/10-kubeadm.conf

# cd /etc/kubernetes/manifests
# ls
total 0
Here the /etc/kubernetes/manifests folder is empty

# docker images

# docker ps

# ps -ef | grep proxy
... /usr/local/bin/kube-proxy ...


On the Master Node:

# kubectl run nginx --image=nginx
deployment "nginx" created

# kubectl expose deployments nginx --port 80 --type NodePort
service "nginx" exposed

# kubectl get pods

# kubectl get svc

On the Worker Node:

# iptables-save | grep nginx


Installing Kubernetes with Kubeadm

Install Kubeadm on all three machines by running the below commands:

# apt-get update && apt-get install -y apt-transport-https curl

# curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -

# cat <<EOF >/etc/apt/sources.list.d/kubernetes.list
deb http://apt.kubernetes.io/ kubernetes-xenial main
EOF

# apt-get update

# apt-get install -y kubelet kubeadm kubectl

# apt-mark hold kubelet kubeadm kubectl

# apt-get install -y docker.io kubernetes-cni

# which kubectl

# which kubeadm

# docker ps


Bootstrapping the cluster

On the Master Node:

# kubeadm init
...
...
You can now join any number of machines by running the following on each node as root:

   kubeadm join --token 883379.2114cc0b3850f21d 138.68.156.233:6443

# mkdir -p $HOME/.kube

# sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

# sudo chown $(id -u):$(id -g) $HOME/.kube/config


On the Worker Nodes:

# kubeadm join --token 883379.2114cc0b3850f21d 138.68.156.233:6443 --skip-preflight-checks

On the Master Node:

# kubectl get nodes
NAME            STATUS      AGE   VERSION
ubuntu-2gb-01   NotReady    2m    v1.7.2

We get this because kubelet on the worker nodes are not running...

On the Worker Nodes:

# systemctl restart kublet

On the Master Node:

# kubectl get nodes
NAME            STATUS      AGE   VERSION
ubuntu-2gb-01   NotReady    3m    v1.7.2
ubuntu-2gb-02   NotReady    19s   v1.7.2
ubuntu-2gb-03   NotReady    30s   v1.7.2

We see NotReady status as the network is not ready, we can setup the network using one of the network add-ons

On the Master Node:

# kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"
serviceaccount "weave-net" created
clusterrole "weave-net" created
clusterrolebinding "weave-net" created
daemonset "weave-net" created

# kubectl get nodes
NAME            STATUS      AGE   VERSION
ubuntu-2gb-01   NotReady    7m    v1.7.2
ubuntu-2gb-02   NotReady    3m    v1.7.2
ubuntu-2gb-03   NotReady    3m    v1.7.2



Kubernetes API:

kubectl get pods
curl -XGET -H "Accept: application/json" https://192.168.99.100:8443/api/v1/namespaces/default/pods

$ kubectl | more

$ kubectl get | more

$ kubectl run ghost --image=ghost
deployment "ghost" created

$ kubectl get pods

$ kubectl -v=9 get pods

$ kubectl delete deployment ghost
deployment "ghost" deleted

$ kubectl get pods

$ kubectl proxy
Starting to serve on 127.0.0.1:8001

$ curl http://localhost:8001/api/v1/namespaces/default/pods
{
  "kind": "PodList",
  "apiVersion": "v1",
  ...
  ...
  "items": []
}

$ cat pod.json
{
  "apiVersion": "v1",
  "metadata": {
    "name": "ghost"
  },
  "kind": "Pod",
  "spec": {
    "containers": [
      {
        "name": "ghost",
        "image": "ghost"
      }
    ]
   }
}
 
$ curl -H "Content-Type: application/json" --data @pod.json -XPOST http://localhost:8001/api/v1/namespaces/default/pods
 
$ curl http://localhost:8001/api/v1/namespaces/default/pods
 
$ kubectl get pods
 
$ curl -XDELETE localhost:8081/api/v1/namespaces/default/pods/ghost
 
$ curl http://localhost:8001/api/v1/namespaces/default/pods
 
$ kubectl get pods
 
 
 
Understanding Namespaces:
 
$ kubectl proxy
Starting to serve on 127.0.0.1:8001

$ curl -s localhost:8001/api/v1/namespaces | jq -r .items[].metadata.name
default
kube-public
kube-system

$ curl -s localhost:8001/api/v1/namespaces

$ kubectl create ns oreilly
namespace "oreilly" created

$ curl -s localhost:8001/api/v1/namespaces | jq -r .items[].metadata.name
default
...
...
oreilly

$ kubectl get ns

$ curl -H "Content-Type: application/json" --data @pod.json -XPOST http://localhost:8001/api/v1/namespaces/default/pods
$ curl -H "Content-Type: application/json" --data @pod.json -XPOST http://localhost:8001/api/v1/namespaces/default/pods
{
...
...
"message": "pods \"ghost\" already exists",
...
"code": 409
}

But we can crete the same pod in orielly namespace...
$ curl -H "Content-Type: application/json" --data @pod.json -XPOST http://localhost:8001/api/v1/namespaces/oreilly/pods

$ kubectl get pods

$ kubectl get pods -n oreilly

$ kubectl get pods --all-namespaces


Quotas:

$ kubectl create quota foobar --hard pods=1
resourcequota "foobar" created

$ kubectl get resourcequotas

$ kubectl get resourcequotas foobar -o yaml

$ cat pod.json
{
  "apiVersion": "v1",
  "metadata": {
    "name": "secondghost"
  },
  "kind": "Pod",
  "spec": {
    "containers": [
      {
        "name": "ghost",
        "image": "ghost"
      }
    ]
  }
}
 
$ curl -H "Content-Type: application/json" --data @pod.json -XPOST http://localhost:8001/api/v1/namespaces/default/pods
{
  "kind": "Status",
  "apiVersion": "v1",
  "metadata": {},
  "status": "Failure",
  "message": "pods \"secondghost" is forbidden: exceeded quota: foobar, requested: pods=1, used: pods=1, limited: pods=1",
  "reason": "Forbidden",
  "details": {
    "name": "secondghost",
    "kind": "pods",
  },
  "code": 403
}

However we can still create the "secondghost" pod in oreilly namespace...
$ curl -H "Content-Type: application/json" --data @pod.json -XPOST http://localhost:8001/api/v1/namespaces/oreilly/pods

$ kubectl get pods --all-namespaces


Using Labels to select resources:

$ cat pod.json
{
  "apiVersion": "v1",
  "metadata": {
    "name": "secondghost"
  },
  "kind": "Pod",
  "spec": {
    "containers": [
      {
        "name": "ghost",
        "image": "ghost"
      }
    ]
  }
}

$ kubectl create -f pod.json
pod "ghost" created

$ kubectl get pods

$ kubectl get pods --show-labels
... ... ... ... LABELS
... ... ... ... <none>

$ kubectl label pods ghost foo=bar
pod "ghost" labeled

$ kubectl get pods --show-labels
... ... ... ... LABELS
... ... ... ... foo=bar

$ kubectl label pods ghost bar=baz
pod "ghost" labeled

$ kubectl get pods --show-labels
... ... ... ... LABELS
... ... ... ... bar=baz,foo=bar

Every resource in Kubernetes can be labeled

$ kubectl get pods ghost -o yaml
kind: Pod
apiVersion: v1
metadata:
  creationTimestamp: 2018-09-21T11:10:40Z
  labels:
    bar: baz
    foo: bar
  name: ghost
  namespace: default
  resourceVersion: "46944"
  ...
...

Get pods by label
$ kubectl get pods -l green=blue

$ kubectl get pods -l foo=bar

$ kubectl get pods -Lfoo


Kubernetes API Schema:

$ kubectl api-versions

$ kubectl proxy

$ curl localhost:8001/api/v1
Some objects are namespaced, and some are not namespaced

$ cat pod.json
{
  "apiVersion": "v1",
  "kind": "Pod",
  "metadata": {
    "name": "ghost"
  },
  "spec": {
    "containers": [
      {
        "name": "ghost",
        "image": "ghost"
      }
    ]
  }
}


Deploying Our First Application:

$ vi pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: ghost
spec:
  containers:
  - name: ghost
    image: ghost

$ kubectl create -f pod.yaml
pod "ghost" created

$ kubectl get pods

$ kubectl delete pods ghost

$ kubectl get pods

$ vi rs.yaml
apiVersion: extensions/v1beta1
kind: ReplicaSet
metadata:
  name: ghost
spec:
  replicas: 5
  selector:
    matchLabels:
      app: ghost
  template:
    metadata:
      name: ghost
      labels:
        app: ghost
    spec:
      containers:
      - name: ghost
        image: ghost

$ kubectl create -f rs.yaml
replicaset "ghost" created

$ kubectl get pods

$ kubectl delete pods ghost-0fd2b
pod "ghost-0fd2b" deleted

$ kubectl get pods

$ vi svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: ghost
spec:
  selector:
    app: ghost
  ports:
  - port: 2368
  type: NodePort

$ kubectl create -f svc.yaml
service "ghost" created

$ kubectl get svc

$ kubectl get rs

$ kubectl scale rs ghost --replicas=2

$ kubectl get rs

$ kubectl get pods

$ kubectl scale rs ghost --replicas=10

$ kubectl get pods -w


Run Generator:

$ kubectl run ghost --image=ghost:1.7 --port 2368 --expose=true
service "ghost" created
deployment "ghost" created

$ kubectl get pods

$ kubectl get rs

$ kubectl get deployments

$ kubectl get svc

$ kubectl edit svc ghost
change the type: to NodePort

$ kubectl get svc


Rolling Update:

$ kubectl run ghost --image=ghost:1.7 --port 2368 --expose=true
service "ghost" created
deployment "ghost" created

$ kubectl scale deployments ghost --replicas=4

$ kubectl get pods

$ kubectl get pods -o json | jq -r .items[].spec.containers[].image
ghost:1.7
ghost:1.7
ghost:1.7
ghost:1.7

$ kubectl edit deployments ghost
change the -image: to ghost:1.8

$ kubect get pods
Now we should see that some pods are in Terminating state, and some pods are in ContainerCreating state

$ kubectl get pods -o json | jq -r .items[].spec.containers[].image
ghost:1.8
ghost:1.8
ghost:1.8
ghost:1.8

$ kubect get rs
Here we should see two replicasets...
Everytime we update a deployment, a new replicaset is created.

$ kubectl get deployments



Helm package manager:

$ kubectl get pods --all-namespaces

$ which helm
/usr/local/bin/helm

$ helm init
...
...
Tiller (the helm server side component) has been installed into your kubernetes Cluster.
Happy Helming!

$ kubectl get pods --all-namespaces

$ helm search redis
...
...

$ helm install stable/redis

$ helm ls

$ kubectl get pods

$ kubectl get secret --namespace default lolling-coral-redis -o jsonpath="{.data.redis-password}" | base64 --decode
........

$ kubectl exec -ti <redis-pod-name> -- redis-cli -a <redis-password>
> info



Extending the Kubernetes API: CRD

$ cat db-crd.yaml
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: databases.foo.bar
spec:
  group: foo.bar
  version: v1
  scope: Namespaced
  names:
    plural: databases
    singular: database
    kind: DataBase
    shortNames:
    - db

$ kubectl create -f db-crd.yaml
customresourcedefinition "databases.foo.bar" created

$ kubectl get crd
...

$ kubectl get databases

$ kubectl get db

$ cat db.yaml
apiVersion: foo.bar/v1
kind: DataBase
metadata:
  name: my-new-db
spec:
  type: mysql

$ kubectl create -f db.yaml
database "my-new-db" created

$ kubectl get db my-new-db -o yaml
apiVersion: foo.bar/v1
kind: DataBase
....

$ 

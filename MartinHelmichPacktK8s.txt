Develop and Operate Microservices on Kubernetes : Martin Helmich

12th September 2018

https://github.com/PacktPublishing/Develop-and-Operate-Microservices-on-Kubernetes

Installing Minikube:

https://github.com/kubernetes/minikube/releases

$ chmod +X ~/downloads/minikube-linux-amd64

$ sudo mv ~/downloads/minikube-linux-amd64 /usr/local/bin/minikube

$ minikube status

$ minikube start

$ minikube stop

$ minikube delete

Minikube Add-Ons:

Add-Ons enabled by default:
> DNS
> Dashboard
> Storage provisioner

Add-Ons NOT enabled by default:
> Ingress
> Heapster

$ minikube addons enable ingress

$ minikube addons enable heapster


Pods - The Basic Deployment Unit
> Multiple containers scheduled on the same node
> Shared network namespaces for all containers in the Pod

Namespaces - Resource Grouping and Isolation
> Purely virtual for grouping resources
> Two namespaces in a fresh Kubernetes cluster: default and kube-system
> No additional isolation at container level

imagePullSecret:
Option for authenticating docker pull from private registries

apiVersion: v1
kind: Pod
metadata:
  name: test-pod
  namespace: default
spec:
  containers:
      name: service
      image: nginx
      ports:
        - containerPort: 80
      env:
        - name: SOME_ENV_VAR
          value: Hello World


> Each Pod is assigned to a specific Node in the cluster
> If a Pod crashes, it will be restarted
> If a Node crashes, Pods running on it will be lost as well


Higher Level Kubernetes Resources:
1. ReplicaSet
2. Deployment
3. StatefulSet

ReplicaSet
> Manages Pods
> Self-healing and Scalable

Deployment
> Manages ReplicaSets
> Rolling updates and rollbacks

StatefulSet
> Similar to ReplicaSet
> Pods have stable network identity
> Pods have a startup order


Labels:
Every object can have arbitrary number of labels
We can use labels to query and filter objects

$ kubectl get pods -l app=my-app


ReplicaSet Properties
1. Pod template
  > Definition for Pods that are managed by this ReplicaSet
2. Replica count
  > Defines how many instances of the Pod should be running at any time
3. Label Selector
  > Determines which Pods are manaed by the ReplicaSet

Example:
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: my-service
spec:
  replicas: 3
  selector:
    matchLabels:
      app: my-service
  template:
    metadata:
      labels:
        app: my-service
    spec:
      containers:
      - name: service
        image: nginx

$ kubectl apply -f replicaset.yaml

Pod Networking:
> Pods have IP addresses
> Pod IPs are routable within the cluster (no external connectivity)
< Pods are short-lived
< Addresses may change frequently
< No DNS for Pod IPs
< No load balancing

Services:
> Selects a set of Pods by label
> One stable IP and DNS name for a group of Pods
> Round-robin load balancing
> Automatically adds/removes Pods as they come and go
> DNS name: my-service.services.cluster.local

Discovering Services:

Environment Variables
Kubelet adds a set of environment variables to the Pod for each active Service in the cluster.
{SVCNAME}_SERVICE_HOST and {SVCNAME}_SERVICE_PORT where the Service name is uppercased and any dashes are converted to underscores
e.g. Service named "redis-master" will create the below environment variables inside the Pods
REDIS_MASTER_SERVICE_HOST=10.0.0.11
REDIS_MASTER_SERVICE_PORT=6379

Any Service that a Pod wants to access MUST BE CREATED BEFORE the Pod itself,
otherwise the environment variables will not be populated

DNS Service Discovery:
Provided by DNS Server cluster add-on which watches the Kubernetes API for new Services
DNS Server creates a set of DNS records for each new Service
All Pods are able to resolve the Service name automatically

E.g. If we have a Service named "my-service" in Namespace "my-ns", then a DNS record for "my-service-my-ns" is created.
Pods in the "my-ns" Namespace are able to do name lookup fo "my-service"
Pods in other Namespaces must qualify the name as "my-service-my-ns"

https://kubernetes.io/docs/concepts/services-networking/service/

https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types

ClusterIP Service:
> Stable internal IP
> Stable internal DNS
> Routed within the cluster
> Can be used by Pods in the cluster to connect to the Pods selected by a Service

NodePort Service:
> A ClusterIP Service is created
> Every Node gets a public TCP port forwarding to that ClusterIP Service

LoadBalancer Service:
> A NodePort Service is created
> Additionally a load balancer is created to allow external incomming traffic


Example:
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  type: ClusterIP
  selector:
    app: my-service
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80

$ kubectl apply -f service.yaml

$ kubectl get services

$ kubectl run --rm -it --image=alpine my-test
# apk -U add curl
...
# curl -v my-service
...
This shows that we can access the service by simply using its name from other Pods in the same Namespace

FQDN: service-name.namespace-name.svc.cluster.local

Example Load Balancer Service:
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  type: LoadBalancer
  selector:
    app: my-service
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80



HTTP Connectivity with Ingress:

Services have few disadvantages:
< Do raw TCP/UDP forwarding
< Do not know about application protocols like HTTP

Ingress:
> Defines request routing for HTTP requests to services
> Works at application layer
> Supports path and host-based routing
> Supports caching, authentication and more

Example:
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: my-ingress
spec:
  rules:
  - host: my-app.example
    http:
      paths:
      - path: /shop
        backend:
          serviceName: my-service
          servicePort: 80
      - path: /home
        backend:
          serviceName: my-other-service
          servicePort: 80

Ingress resource can have multiple Host rules
One Host rule per HTTP host name

Each Host rule can have multiple Path rules
Each Path rule forwards incoming HTTP request to a specific Service resource
Service needs to be defined separately

Ingress Controllers:
Ingress definitions are purely virtual
They require an Ingress Controller that accepts HTTP connections and forwards request where they should go
i.e. cluster must have an Ingress Controller running in order for the Ingress resource to work

ingress-nginx, ingress-gce, F5 BIG-IP, kong, and Traefik are the Ingress Controllers currently available.

$ kubectl apply -f ingress.yaml

$ kubectl cluster-info

$ minikube addons list

$ minikube addons enable ingress

$ minikube ip

Ingress Controller Features:
> TLS Offloading (This frees us from having to manage Keys and Certificates within our own Pods)
> Rate Limiting
> Authentication


Managing Application Lifecycle with Deployments:

ReplicaSet Limitations:
< When a ReplicaSet creates a new Pod, it will use the Pod template defined in its specification
< If we change the specification, the ReplicaSet controller will not update any of the existing Pods
< New Pods are created with new image, Existing Pods are not updated
< Only way to update ReplicaSet is to delete and recreate

Deployment Objects:
> Rolling Updates:
> Deployments manage multiple versions of a ReplicaSet
> On update of specification, a new ReplicaSet is created and gradually scaled up
> The old ReplicaSet is gradually scaled down

Example:
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-service
spec:
  replicas: 3
  selector:
    matchLabels:
      app: my-service
    template:
      metadata:
        labels:
          app: my-service
      spec:
        containers:
        - name: service
          image: nginx:1.13.11

$ kubectl apply -f deployment.yaml

$ kubectl get pods

$ kubectl get replicaset

$ kubectl edit deployment my-deployment
Change the Container Image Version....

$ kubectl get pods

$ kubectl get replicasets

$ kubectl rollout undo deployment/my-deployment

$ kubectl get replicasets



Stateful Services and Storage Drivers:

Stateful Pods:
> Pods that have state/need to store data
> Require persistent file system across Pod termination/recreation

Persistent Volume:
> Can be provided as a file system to Pods
> Lifecycle is independent from Pods
> Can be accessed by multiple Pods at once (dependent on storage engine)

Choosing a Network Storage Technology:

Network Block Devices:
E.g. AWS EBS, AzureDisk, GCE Persistent Disk, OpenStack Cinder, Ceph, or iSCSI

Key features:
> Fast
> Fixed-Size
> Can (usually) be used by one Pod at a time


Network File System:
E.g. AWS EFS (actually NFS), Azure Files (actually SMB), or GCE Cloud Filestore, CephFS, GlusterFS, NFS

Key features:
> Slower than Block Devices
> Can be used by multiple Pods at the same time

https://cloud.google.com/filestore/docs/accessing-fileshares



Working with Persistent Volumes:

PersistentVolume: Refers to a specific network block device or file system
PersistentVolumeClaim: Claims a PV for use in one or more Pods

Personas in PV and PVC

Cluster / Storage Operator:
> Knoledgeable in storage architecture
> Knows not much about application architecture
> Creates PV

Application Operator:
> Knows about application architecture
> Not familiar with cluster and storage operations
> Creates PVC, Pod, Deployment, ReplicaSet, and StatefulSet


PersistentVolume Example:
apiVersion: v1
kind: PersistentVolume
metadata:
  name: my-volume
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  awsElasticBlockStore:
    volumeID: vol-08374e22abe6cb9b6
    fsType: ext4

accessModes: can also have ReadWriteMany that allows many Pods to read and write at same time


PersistentVolumeClaim Example:
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-volume-claim
spec:
  resources:
    requests:
      storage: 10Gi
  accessModes:
    - ReadWriteOnce


We can also use Label Selector with PV and PVC
i.e. Label the PV with certain labels and use those labels in PVC Label Selector

Once we have created a PersistentVolume and PersistentVolumeClaim,
we can use the PersistentVolumeClaim in a Pod or StatefulSet

apiVersion: v1
kind: Pod
metadata:
  name: my-database
spec:
  containers:
    - name: database
      image: mysql:5.7
      env:
        #...
      volumeMounts:
        - mountPath: /var/lib/mysql
          name: data
  volumes:
    - name: data
      persistentVolumeClaim:
        claimName: my-volume-claim


$ kubectl apply -f persistentvolume.yaml
persistentvolume "my-volume" created

$ kubectl get pv
NAME        CAPACITY   ...
my-volume   10Gi       ...

$ kubectl apply -f persistentvolumeclaim.yaml
persistentvolumeclaim "my-volume-claim" created

$ kubectl get pvc
NAME              STATUS   VOLUME                                CAPACITY   ACCESS MODES   ...
my-volume-claim   Bound    pvc-7f9c4429-6777-11e8-080027c553e0   5Gi        RWO            ...

$ kubectl apply -f pod.yaml
pod "my-database" created

$ kubectl get pods
If this Pod is deleted and recreated, it would still use the same PVC and persist data


Automatic Volume Provisioning: Storage Class

Cluster / Storage Operator:
Creates -> StorageClass

Application Operator:
Creates -> PersistentVolumeClaim
In this case PersistentVolumeClaim references the Storage Class
This is picked up by Kubernetes Volume Provisioner to create a new PersistentVolume
The Kubernetes Volume Provisioner also bind the PV with PVC

StorageClass Example:
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: ssd-fast
provisioner: kubernetes.io/gce-pd
parameters:
  type: io1
  zones: us-central1
  iopsPerGB: "20"

Values in the parameters: section here are Provisioner dependent

PersistentVolumeClaim Example with StorageClass:
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-volume-claim
spec:
  storageClass: ssd-fast
  resources:
    requests:
      storage: 10Gi
  accessModes:
    - ReadWriteOnce

$ kubectl get storageclass

$ kubectl get pvc

$ kubectl get pv



StatefulSets:
> Manage Pods and Volumes
> Self-healing and scalable
> Ideal for clustered stateful apps

Headless Service:
> Resolve StatefulSet via DNS
> Resolve individual members via DNS
> NO load balancing, DNS only

StatefulSet Example:
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-database
spec:
  replicas: 3
  selector:
    matchLabels:
      app: my-database
  service: my-db-service
  template:
    metadata:
      labels:
        app: my-database
    spec:
      containers:
      - name: db
        image: mongo:3.7.9
        ports:
        #...
        volumeMount:
        - name: data
          mountPath: /data
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [ReadWriteOnce]
      storageClassName: ssd-fast
      resources:
        requests:
          storage: 10Gi


Headless Services Example:
apiVersion: v1
kind: Service
metadata:
  name: my-db-service
spec:
  ports:
  - ports: 27017
    name: mongodb
  clusterIP: None
  selector:
    app: my-database

Here the "clusterIP: None" specifies Only DNS, NO IP load balancing

$ kubectl apply -f headless-service.yaml

$ kubectl apply -f statefulset.yaml

$ kubectl get pods
Here we will see three Pods, my-database-0, my-database-1, and my-database-2

$ kubectl get pvc

$ kubect run --rm -it --image alpine test
# nslookup my-db-service
...
Name:      my-db-service
Address 1: 172.17.0.9 my-database-2.my-db-service.default.svc.cluster.local
Address 2: 172.17.0.7 my-database-0.my-db-service.default.svc.cluster.local
Address 3: 172.17.0.4 my-database-1.my-db-service.default.svc.cluster.local
#
Here we can see that the service name "my-db-service" resolves to individual IPs of the three Pods


StatefulSets Vs Deployments
> Stable identity, DNS-addressable
< No stable identity

> Pods are started and stopped in order
< Pods are started / stopped all at once, or randomly in case of scaling

> Volume provisioning for new members
< Manually configured shared volumes


ConfigMap:
> Can contain arbitrary key/value pairs
> Can be used by multiple Pods at once
> Can be updated independently of the Pod

Example ConfigMap contents:
data:
  someVariable: "some value"

In the Pod definition...
env:
- name: SOME_VARIABLE
  valueFrom:
    configMapKeyRef:
      name: my-configmap
      key: someVariable

ConfigMap Environment Variable Example:
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-configmap
data:
  someVariable: "some value"
  someBoolean: "true"

NOTE: Values of ConfigMap key/value pair must be STRINGS

ConfigMap Environment Variable Usage Example:
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
  - name: my-service
    image: nginx:1.3.12
    env:
    - name: SOME_VARIABLE
      valueFrom:
        confiMapKeyRef:
          name: my-configmap
          key: someVariable

ConfigMap "my-configmap" must exist when we create the Pod,
otherwise the Pod will go in to Pending state

Environment Variable value is set when the Pod is started,
any updates to the ConfigMap after the Pod is started are not reflected in already running Pods

$ kubectl apply -f configmap.yaml

$ kubectl get configmaps

$ vi pod.yaml
kind: Pod
apiVersion: v1
metadata:
  name: my-pod
spec:
  containers:
  - name: app
    image: alpine
    command: ["sleep", "3600"]
    env:
    - name: SOME_VARIABLE
      valuFrom:
        configMapKeyRef:
          name: my-configmap
          key: somevariable

$ kubectl apply -f pod.yaml
pod "my-pod" created

$ kubectl exec -it my-pod sh
# env
...
...
SOME_VARIABLE=some value
...


Injecting Configuration Files into Pods:

ConfigMap Example:
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-configmap
data:
  config.ini: |
    some_variable = foo
    some_other_variable = baz
  database.ini: |
    uri = mongodb://my-database:27017/test

ConfigMap Usage Example:
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
  - name: my-service
    image: nginx:1.3.12
    volumeMounts:
    - name: config
      mountPath: /etc/config
  volumes:
  - name: config
    configMap:
      name: my-configmap

NOTE: ConfigMap volumes are mounted READ-ONLY

NOTE: volumeMount is AUTOMATICALLY UPDATED when the ConfigMap changes
This is not the case when using confiMapKeyRef to set Environment Variables in the Pod

$ ls
config.ini database.ini

$ kubectl create configmap my-configmap --from-file=.
configmap "my-configmap" created

$ kubectl edit configmap my-configmap

$ kubectl apply -f pod.yml
pod "my-pod" created

$ kubectl exec -it my-pod sh
# ls -l /etc/config
config.ini -> ..data/config.ini
database.ini -> ..data/database.ini
# cat /etc/config/config.ini
some_variable = foo
some_other_variable = baz


Secrets:
> Very similar to ConfigMap
> Encryption at rest possible (not default yet)
> Strict access control possible

Secret Example:
apiVersion: v1
kind: Secret
metadata:
  name: my-secret
data:
  test: VGVzdA==

On the storage level, secrets are stored base64 encoded

NOTE: base64 is NOT ENCRYPTION !!!

Secrets Usage in Environment Variables:
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
  - name: my-service
    image: nginx:1.3.12
    env:
    - name: SOME_SECRET_VARIABLE
      valueFrom:
        secretKeyRef:
          name: my-secret
          key: someVariable


Secrets Usage in Volumes:
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
  - name: my-service
    image: nginx:1.3.12
    volumeMounts:
    - name: secret
      mountPath: /etc/secret
  volumes:
  - name: secret
    secret:
      name: my-secret

NOTE: Values of secret key/value pair MUST BE base64 ENCODED!!!

$ kubectl create secret generic my-secret --from-file=.
This command will import all files in the current directory and their content in to a new secret object

$ kubectl edit secret my-secret

$ kubectl create secret generic my-other-secret --from-literal=someVariable=someValue


Composing Environment Variables:

apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
  - name: my-service
    image: nginx:1.3.12
    env:
    - name: MONGODB_PASSWORD
      valueFrom:
        secretKeyRef:
          name: mongodb-credentials
          key: password
    - name: MONGODB_URL
      value: "mongodb://user:$(MONGODB_PASSWORD)@mongo/database"
      
Here we have used MONGODB_PASSWORD Environment variable to compose MONGODB_URL Environment variable



Continous Delivery with Kubernetes:

$ kubectl set image deployment/my-deployment app=app:v1.5
This command updates the deployment specification to use the new image.
Updating the image will cause the deployment controller to start the Rolling Update

NOTE: "kubectl set image" command requires that Deployment already exists

$ kubectl set image deployments --namespace=some-namespace --dry-run --selector app=worker worker=image:newtag


Helm Architecture:

Helm Server (Tiller)
> Runs in Kubernetes cluster (kube-system namespace)
> Quick setup (helm init command)

Helm Charts:
> Portable deployment definitions
> Chart can be provided by us for our own applications
> Charts are available from many third party providers

Helm Client:
> Runs on your machine (or CI server)
> Reads charts and deploys them to Helm server
> Uses the charts and creates Releases on the server

Each Release then creates kubernetes resources like Pods, Deployments, StatefulSets, Services, and Ingreses
Each chart can be released multiple times within the same Kubernetes cluster
Helm can also update existing releases when a new version of the chart is available

Download & Install Helm:
https://docs.helm.sh/using_helm/#installing-helm

$ helm init

$ kubectl get pods --namespace kube-system
...
tiller-deploy-587df449fb-tgt5f   1/1   Running   0   12s

Helm Chart File Structure:

my-chart/
    Chart.yaml
    values.yaml
    templates/
        deployment.yaml
        service.yaml

Chart.yaml
Contains general metadata about the helm chart, like name and version
Version here is the version of the chart, and not of the application defined by the chart

values.yaml
Contains parameters configurable by the user when releasing helm
These values are then used in the helm templates

Templates
Kubernetes resource definitions in the form of template files
Templates help us to reuse the values from values.yaml file
Templates uses golang templates https://golang.org/pkg/text/template/

We can create these files for a new project by using the below command
$ helm create <name>

$ ls
Dockerfile, chart, example-app, main.go

$ helm install --name myrelease ./chart

We can also use the --set flag to override any values from the values.yaml
$ helm install --set image.tag=v1.0.0 --name myrelease ./chart

We can also use helm to upgrade existing releases
$ helm upgrade myrelease ./chart

We can override values from values.yaml in helm upgrade using --set flag
$ helm upgrade --set image.tag=v1.0.1 myrelease ./chart

We can combine helm install and upgrade by using --install flag
$ helm upgrade --install --set image.tag=v1.0.1 myrelease ./chart
This command will upgrade the release, and create it if it does not exist already




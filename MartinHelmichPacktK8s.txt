Develop and Operate Microservices on Kubernetes : Martin Helmich

12th September 2018

https://github.com/PacktPublishing/Develop-and-Operate-Microservices-on-Kubernetes

Installing Minikube:

https://github.com/kubernetes/minikube/releases

$ chmod +X ~/downloads/minikube-linux-amd64

$ sudo mv ~/downloads/minikube-linux-amd64 /usr/local/bin/minikube

$ minikube status

$ minikube start

$ minikube stop

$ minikube delete

Minikube Add-Ons:

Add-Ons enabled by default:
> DNS
> Dashboard
> Storage provisioner

Add-Ons NOT enabled by default:
> Ingress
> Heapster

$ minikube addons enable ingress

$ minikube addons enable heapster


Pods - The Basic Deployment Unit
> Multiple containers scheduled on the same node
> Shared network namespaces for all containers in the Pod

Namespaces - Resource Grouping and Isolation
> Purely virtual for grouping resources
> Two namespaces in a fresh Kubernetes cluster: default and kube-system
> No additional isolation at container level

imagePullSecret:
Option for authenticating docker pull from private registries

apiVersion: v1
kind: Pod
metadata:
  name: test-pod
  namespace: default
spec:
  containers:
      name: service
      image: nginx
      ports:
        - containerPort: 80
      env:
        - name: SOME_ENV_VAR
          value: Hello World


> Each Pod is assigned to a specific Node in the cluster
> If a Pod crashes, it will be restarted
> If a Node crashes, Pods running on it will be lost as well


Higher Level Kubernetes Resources:
1. ReplicaSet
2. Deployment
3. StatefulSet

ReplicaSet
> Manages Pods
> Self-healing and Scalable

Deployment
> Manages ReplicaSets
> Rolling updates and rollbacks

StatefulSet
> Similar to ReplicaSet
> Pods have stable network identity
> Pods have a startup order


Labels:
Every object can have arbitrary number of labels
We can use labels to query and filter objects

$ kubectl get pods -l app=my-app


ReplicaSet Properties
1. Pod template
  > Definition for Pods that are managed by this ReplicaSet
2. Replica count
  > Defines how many instances of the Pod should be running at any time
3. Label Selector
  > Determines which Pods are manaed by the ReplicaSet

Example:
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: my-service
spec:
  replicas: 3
  selector:
    matchLabels:
      app: my-service
  template:
    metadata:
      labels:
        app: my-service
    spec:
      containers:
      - name: service
        image: nginx

$ kubectl apply -f replicaset.yaml

Pod Networking:
> Pods have IP addresses
> Pod IPs are routable within the cluster (no external connectivity)
< Pods are short-lived
< Addresses may change frequently
< No DNS for Pod IPs
< No load balancing

Services:
> Selects a set of Pods by label
> One stable IP and DNS name for a group of Pods
> Round-robin load balancing
> Automatically adds/removes Pods as they come and go
> DNS name: my-service.services.cluster.local

Discovering Services:

Environment Variables
Kubelet adds a set of environment variables to the Pod for each active Service in the cluster.
{SVCNAME}_SERVICE_HOST and {SVCNAME}_SERVICE_PORT where the Service name is uppercased and any dashes are converted to underscores
e.g. Service named "redis-master" will create the below environment variables inside the Pods
REDIS_MASTER_SERVICE_HOST=10.0.0.11
REDIS_MASTER_SERVICE_PORT=6379

Any Service that a Pod wants to access MUST BE CREATED BEFORE the Pod itself,
otherwise the environment variables will not be populated

DNS Service Discovery:
Provided by DNS Server cluster add-on which watches the Kubernetes API for new Services
DNS Server creates a set of DNS records for each new Service
All Pods are able to resolve the Service name automatically

E.g. If we have a Service named "my-service" in Namespace "my-ns", then a DNS record for "my-service-my-ns" is created.
Pods in the "my-ns" Namespace are able to do name lookup fo "my-service"
Pods in other Namespaces must qualify the name as "my-service-my-ns"

https://kubernetes.io/docs/concepts/services-networking/service/

https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types

ClusterIP Service:
> Stable internal IP
> Stable internal DNS
> Routed within the cluster
> Can be used by Pods in the cluster to connect to the Pods selected by a Service

NodePort Service:
> A ClusterIP Service is created
> Every Node gets a public TCP port forwarding to that ClusterIP Service

LoadBalancer Service:
> A NodePort Service is created
> Additionally a load balancer is created to allow external incomming traffic


Example:
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  type: ClusterIP
  selector:
    app: my-service
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80

$ kubectl apply -f service.yaml

$ kubectl get services

$ kubectl run --rm -it --image=alpine my-test
# apk -U add curl
...
# curl -v my-service
...
This shows that we can access the service by simply using its name from other Pods in the same Namespace

FQDN: service-name.namespace-name.svc.cluster.local

Example Load Balancer Service:
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  type: LoadBalancer
  selector:
    app: my-service
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80



HTTP Connectivity with Ingress:

Services have few disadvantages:
< Do raw TCP/UDP forwarding
< Do not know about application protocols like HTTP

Ingress:
> Defines request routing for HTTP requests to services
> Works at application layer
> Supports path and host-based routing
> Supports caching, authentication and more

Example:
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: my-ingress
spec:
  rules:
  - host: my-app.example
    http:
      paths:
      - path: /shop
        backend:
          serviceName: my-service
          servicePort: 80
      - path: /home
        backend:
          serviceName: my-other-service
          servicePort: 80

Ingress resource can have multiple Host rules
One Host rule per HTTP host name

Each Host rule can have multiple Path rules
Each Path rule forwards incoming HTTP request to a specific Service resource
Service needs to be defined separately

Ingress Controllers:
Ingress definitions are purely virtual
They require an Ingress Controller that accepts HTTP connections and forwards request where they should go
i.e. cluster must have an Ingress Controller running in order for the Ingress resource to work

ingress-nginx, ingress-gce, F5 BIG-IP, kong, and Traefik are the Ingress Controllers currently available.

$ kubectl apply -f ingress.yaml

$ kubectl cluster-info

$ minikube addons list

$ minikube addons enable ingress

$ minikube ip

Ingress Controller Features:
> TLS Offloading (This frees us from having to manage Keys and Certificates within our own Pods)
> Rate Limiting
> Authentication


Managing Application Lifecycle with Deployments:

ReplicaSet Limitations:
< When a ReplicaSet creates a new Pod, it will use the Pod template defined in its specification
< If we change the specification, the ReplicaSet controller will not update any of the existing Pods
< New Pods are created with new image, Existing Pods are not updated
< Only way to update ReplicaSet is to delete and recreate

Deployment Objects:
> Rolling Updates:
> Deployments manage multiple versions of a ReplicaSet
> On update of specification, a new ReplicaSet is created and gradually scaled up
> The old ReplicaSet is gradually scaled down

Example:
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-service
spec:
  replicas: 3
  selector:
    matchLabels:
      app: my-service
    template:
      metadata:
        labels:
          app: my-service
      spec:
        containers:
        - name: service
          image: nginx:1.13.11

$ kubectl apply -f deployment.yaml

$ kubectl get pods

$ kubectl get replicaset

$ kubectl edit deployment my-deployment
Change the Container Image Version....

$ kubectl get pods

$ kubectl get replicasets

$ kubectl rollout undo deployment/my-deployment

$ kubectl get replicasets



Stateful Services and Storage Drivers:

Stateful Pods:
> Pods that have state/need to store data
> Require persistent file system across Pod termination/recreation

Persistent Volume:
> Can be provided as a file system to Pods
> Lifecycle is independent from Pods
> Can be accessed by multiple Pods at once (dependent on storage engine)

Choosing a Network Storage Technology:

Network Block Devices:
E.g. AWS EBS, AzureDisk, GCE Persistent Disk, OpenStack Cinder, Ceph, or iSCSI

Key features:
> Fast
> Fixed-Size
> Can (usually) be used by one Pod at a time


Network File System:
E.g. AWS EFS (actually NFS), Azure Files (actually SMB), or GCE Cloud Filestore, CephFS, GlusterFS, NFS

Key features:
> Slower than Block Devices
> Can be used by multiple Pods at the same time

https://cloud.google.com/filestore/docs/accessing-fileshares



Working with Persistent Volumes:

PersistentVolume: Refers to a specific network block device or file system
PersistentVolumeClaim: Claims a PV for use in one or more Pods

Personas in PV and PVC

Cluster / Storage Operator:
> Knoledgeable in storage architecture
> Knows not much about application architecture
> Creates PV

Application Operator:
> Knows about application architecture
> Not familiar with cluster and storage operations
> Creates PVC, Pod, Deployment, ReplicaSet, and StatefulSet


PersistentVolume Example:
apiVersion: v1
kind: PersistentVolume
metadata:
  name: my-volume
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  awsElasticBlockStore:
    volumeID: vol-08374e22abe6cb9b6
    fsType: ext4

accessModes: can also have ReadWriteMany that allows many Pods to read and write at same time


PersistentVolumeClaim Example:
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-volume-claim
spec:
  resources:
    requests:
      storage: 10Gi
  accessModes:
    - ReadWriteOnce


We can also use Label Selector with PV and PVC
i.e. Label the PV with certain labels and use those labels in PVC Label Selector

Once we have created a PersistentVolume and PersistentVolumeClaim,
we can use the PersistentVolumeClaim in a Pod or StatefulSet

apiVersion: v1
kind: Pod
metadata:
  name: my-database
spec:
  containers:
    - name: database
      image: mysql:5.7
      env:
        #...
      volumeMounts:
        - mountPath: /var/lib/mysql
          name: data
  volumes:
    - name: data
      persistentVolumeClaim:
        claimName: my-volume-claim


$ kubectl apply -f persistentvolume.yaml
persistentvolume "my-volume" created

$ kubectl get pv
NAME        CAPACITY   ...
my-volume   10Gi       ...

$ kubectl apply -f persistentvolumeclaim.yaml
persistentvolumeclaim "my-volume-claim" created

$ kubectl get pvc
NAME              STATUS   VOLUME                                CAPACITY   ACCESS MODES   ...
my-volume-claim   Bound    pvc-7f9c4429-6777-11e8-080027c553e0   5Gi        RWO            ...

$ kubectl apply -f pod.yaml
pod "my-database" created

$ kubectl get pods
If this Pod is deleted and recreated, it would still use the same PVC and persist data


Automatic Volume Provisioning: Storage Class

Cluster / Storage Operator:
Creates -> StorageClass

Application Operator:
Creates -> PersistentVolumeClaim
In this case PersistentVolumeClaim references the Storage Class
This is picked up by Kubernetes Volume Provisioner to create a new PersistentVolume
The Kubernetes Volume Provisioner also bind the PV with PVC

StorageClass Example:
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: ssd-fast
provisioner: kubernetes.io/gce-pd
parameters:
  type: io1
  zones: us-central1
  iopsPerGB: "20"


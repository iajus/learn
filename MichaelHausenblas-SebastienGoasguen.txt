Kubernetes Cookbook:

Deploymnet and Service from CLI:

$ kubectl run ghost --image=ghost:0.9
$ kubectl expose deployments ghost --port=2368 --type=NodePort


Get multiple Kubernetes resources

$ kubectl get pods,rs,deployments


Installing kubeadm:
We need kubeadm installed on all the servers that will be part of our Kubernetes cluster.

If using Ubuntu, on each of the hosts run the following commands as root to setup Kubernetes package repository

# apt-get update && apt-get install -y apt-transport-https

# curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -

# cat <<EOF >/etc/apt/sources.list.d/kubernetes.list
  deb http://apt.kubernetes.io/ kubernetes-xenial main
  EOF

# apt-get update

Now we can install Docker, kubeadm, kubectl, kubelet, and kubernetes-cni

# apt-get install -y docker.io
# apt-get install -y kubelet kubeadm kubectl kubernetes-cni

Bootstrapping a Kubernetes Cluster using kubeadm

On Master Node:
# kubeadm init
...
...

On other nodes:
# kubeadm join --token <token>

On Master Node:
$ kubectl get nodes
Here we will see our nodes join the cluster

Final step is to create a network that satisfies Kubernetes Networking Requirements - single IP per Pod
We can use any of the network add-ons, however Weave Net can be installed on Kubernetes v1.6.0 and above with a single command like below.

$ export kubever=$(kubectl version | base64 | tr -d '\n')
$ kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$kubever"

This command will create daemonset running on all nodes in the cluster.
Pods of the daemonset use the host network and a CNI plug-in to configure the local node network.

Our nodes will enter READY state once the network is in place.


Writing systemd Unit File to Run Kubernetes Components:

If we look closely at the kubeadm configuration,
we will see that kubelet running on every node in our cluster, including the master node is managed by systemd

Log in to any of the nodes in the cluster we built using kubeadm

# systemctl status kubelet
kubelet.service - kubelet: The Kubernetes Node Agent
 Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor present: enabled)
Drop-In: /etc/systemd/system/kubelet.service.d
         \__10-kubeadm.conf
...
...

This gives us a link to the systemd unit file: /lib/systemd/system/kubelet.service
And its configuration in /etc/systemd/system/kubelet.service.d/10-kubeadm.conf

The unit file points to kubelet binary installed at /usr/bin/kubelet
The configuration file has information on how the kubelet binary should be started

All the options specified in the configuration file, such as --kubeconfig, defined by environment variable $KUBELET_CONFIG_ARGS,
are startup options of the kubelet binary

Kubelet Reference:
https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/

--pod-manifest-path=/etc/kubernetes/manifests path defined by environment variable $KUBELET_SYSTEM_PODS_ARG
is the path where kubelet will look for manifests that it will automatically start.

# ls /etc/kubernetes/manifests
etcd.yaml, kube-apiserver.yaml, kube-controller-manager.yaml, kube-scheduler.yaml


Using Kubernetes Client:

Listing Resources:

$ kubectl get pods

$ kubectl get deployments,services

$ kubectl get deployment my-deployment

$ kubectl get all

Resource Short Names:
configmaps : cm
daemonsets : ds
deployments : deploy
endpoints : ep
events : ev
horizontalpodautoscaler : hpa
ingresses : ing
namespaces : ns
nodes : no
pods : po
persistentvolumes : pv
persistentvolumeclaims : pvc
replicasets : rs
replicationcontrollers : rc
resourcequotas : quota
serviceaccounts : sa
services : svc

Deleting Resources:

Delete all resources in a namespace:
$ kubectl get ns
...
my-app   Active   20m

$ kubectl delete ns my-app
namespace "my-app" deleted

Delete resources labeled with app=my-app
$ kubectl delete svc,deploy -l app=my-app

Force delete a pod:
$ kubectl delete pod hangingpod --grace-period=0 --force

Delete all pods in a namespace
$ kubectl delete pods --all --namespace test

Watch changes to Kubernetes objects in an interactive manner:
$ kubectl get pods --watch

alternatively we can use the watch command like below..
$ watch kubectl get pods

Editing Resources with kubectl
$ kubectl run nginx --image=nginx
$ kubectl edit deploy/nginx
...
deployment "nginx" edited
NOTE: NOT ALL CHANGES TRIGGER A DEPLOYMENT

Explain resources and Fields:
$ kubectl explain svc

$ kubectl explain svc.spec.externalIPs

KUBECTL EXPLAIN: https://blog.heptio.com/kubectl-explain-heptioprotip-ee883992a243

HEPTIO PROTIP: https://blog.heptio.com/tagged/heptioprotip


Creating and Modifying Fundamental Workloads:

Creating a Deployment Using kubectl run:
$ kubectl run ghost --image=ghost:0.9

$ kubectl get deploy/ghost

kubectl run command takes a number of argumensts to configure additional parameters of the deployment.

--env Set environment variables
--port Define container ports
--command Define a command to run using
--expose Automatically create an associated service
--replicas Define the number of pods

$ kubectl run ghost --image=ghost:0.9 --port=2368 --expose

$ kubectl run mysql --image=mysql:5.5 --env=MYSQL_ROOT_PASSWORD=root

$ kubectl run myshell --image=busybox --comman -- sh -c "sleep 3600"

$ kubectl run --help

Creating Objects from Manifest files:

$ cat myns.yaml
apiVersion: v1
kind: namespace
metadata:
  name: myns

$ kubectl create -f myns.yaml

We can also point kubectl create to a URL instead of a file.

$ kubectl create -f https://path/to/manifest/file


Pod Manifest:
Pod is an /api/v1 obect, Pod manifest contains the following fields

> apiVersion : Specifies the API version
> kind : indicates the type of the object
> metadat : provides some metadata about the object
> spec : provides the object specification

A Pod can contain multiple containers as in the below example.

apiVersion: v1
kind: Pod
metadata:
  name: oreilly
spec:
  containers:
  - name: oreilly
    image: nginx
  - name: safari
    image: redis


Deployment from a Manifest file:

$ cat fancyapp.yaml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: fancyapp
spec:
  replicas: 5
  template:
    metadata:
      labels:
        app: fancyapp
        env: development
    spec:
      containers:
      - name: sise
        image: mhausenblas/simpleservice:0.5.0
        ports:
        - containerPort: 9876
        env:
        - name: SIMPLE_SERVICE_VERSION
          value: "0.9"

$ kubectl create -f fancyapp.yaml

$ kubectl get deploy

$ kubectl get rs

$ kubectl get po

Generating manifest file with kubectl create:

$ kubectl create deployment fancyapp --image nginx -o yaml --dry-run


Updating Deployment:

$ kubectl run my-deployment --image=mhausenblas/simpleservice:0.4.0
deployment "my-deployment" created

$ kubectl set image deployment my-deployment mhausenblas/simpleservice:0.5.0
deployment "my-deployment" image updated

$ kubectl rollout status deployment my-deployment
deployment "my-deployment" successfully rolled out

$ kubectl rollout history deployment my-deployment

$ kubectl rollout undo deployment my-deployment --to-rivision=2

kubectl create with --record flang will record events that trigger a revision.
------------------------------------------------------------------------------

Apply, Patch, and Replace:

kubectl apply : Used to update a deployment from a manifest file or create the deployment if it doesn't exist
e.g. kubectl apply -f my-deployment.yaml

kubectl replace : Used to replace an existing deployment from a manifest file.
e.g. kubectl replace -f my-deployment.yaml

kubeclt patch: Used to update a specific key in the deployment
e.g. kubectl patch deployments mydep -p '{"spec": {"template": {"spec": {"containers": [{"name": "mydep", "image": "myimage:1.5"}]}}}}'


Working with Services:

Creating a Service to expose our application:
$ kubectl expose deploy/nginx --port 80
service "nginx" exposed

$ kubectl describe svc/nginx

$ kubectl get svc | grep nginx

$ kubectl proxy
Starting to serve on 127.0.0.1:8001

Browse to http://localhost:8001/api/v1/proxy/namespaces/default/service/nginx

$ cat nginx-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx
spec:
  selector:
    run: nginx
  ports:
  - port: 80

One thig to pay attention in this YAML file is the "selector"
"selector" is used to select the Pods that make up the microservice abstraction.
Service object dynamically configures "iptables" on all nodes to be able to send traffic to containers that make up the microservice.

$ kubectl get endpoints

Verify the DNS entry of a Service:
By default Kubernetes uses ClusterIP as the Service type, and that exposes the Service on a cluster-internal IP.
If DNS cluster add-on is available and working properly, then we can access the service via a Fully Qualified Domain Name (FQDN)
FQDN is in the form of <Service-Name>.<Namespace>.svc.cluster.local.

To verify the DNS entry of a Service, we can do the below.
$ kubectl run busybox --image busybox -it -- /bin/sh
# nslookup nginx
Server:    10.96.0.10
Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local

Name:      nginx
Address 1: 10.109.24.56 nginx.default.svc.cluster.local

NOTE: Here the IP address returned for the Service should correspond to its ClusterIP

Changing the Type of a Service:
$ cat nginx-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: webserver
spec:
  ports:
  - port: 80
  selector:
    app: nginx

$ kubectl create -f nginx-svc.yaml

$ kubectl get svc/webserver
NAME       CLUSTER-IP  EXTERNAL-IP  PORT(S)  AGE
webserver  10.0.0.39   <none>       80/TCP   56s

$ kubectl get svc/webserver -o yaml

Let's change the Service Type to NodePort:

$ kubectl edit svc/webserver
Change "type:" value to NodePort and containerPort: to nodePort:

$ kubectl get svc/webserver

$ kubectl get svc/webserver -o yaml


Making Sevices Accessible from Outside the Cluster (Ingress Resource):

$ cat ingress-nginx.yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: nginx-public
  annotations:
    ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - host:
    http:
      paths:
      - path: /web
        backend:
          serviceName: nginx
          servicePort: 80

$ kubectl create -f ingress-nginx.yaml

$ kubectl get ingress

$ curl -k https://<Ingress IP>/web

Kubernetes Services and Ingress Under X-ray: http://containerops.org/2017/01/30/kubernetes-services-and-ingress-under-x-ray/



Exploring Kubernetes API and Key Metadata:

$ kubectl proxy --port=80 --api-prefix=/

$ curl localhost/api/v1
...
{
    "name": "pods",
    "namespaced": true,
    "kind": "Pod",
    "verbs": [
      "create",
      "delete",
      "deletecollection",
      "get",
      "list",
      "patch",
      "proxy",
      "update",
      "watch"
    ],
    "shortNames": [
      "po"
    ]
},
...

When discovering the API endpoints, we will see different ones like...

/api/v1

/apis/apps

/apis/authentication.k8s.io

/apis/authorization.k8s.io

/apis/autoscaling

/apis/batch

Each of these endpoints correspond to an API group.
Within a group, API objects are versioned to indicate the maturity of the objects (e.g. v1beta1, v1beta2)

Example:
Pods, Services, ConfigMaps, and Secrets are part of /api/v1 API group
Deployments are part of /apis/extensions/v1beta1 API group

Understanding the Structure of a Kubernetes Manifest:

>All API resources are either Objects or Lists
>All resources have a kind, and an apiVersion
>Every Object kind must have metadata.
>The metadata contains the name of the Object, the namespace the Object is in
>The metadata might also additionaly contain some labels and annotations
>To complete a manifest, most Objects will have a spec
>Once an Object is created, it will also return a status


Namespaces to avoid Name Collision:

$ kubectl create namespace my-app

$ kubectl get ns

$ cat ns.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: my-app

$ kubectl create -f ns.yaml

$ kubectl run foobar --image=ghost:0.9
deployment "foobar" created

$ kubectl run foobar --image=ghost:1.13
Error from server (AlreadyExists): deployments.extensions "foobar" already exists

$ kubectl run foobar --image=ghost:1.13 --namespace foobar
deployment "foobar" created

NOTE: API Objects in Kubernetes are Namespaced

kube-system namespace is reserved for administrators.
kube-public namespace is meant to store public objects available to all users of the cluster.


Setting Quotas Within a Namespace (ResourceQuota):

$ cat resource-quota-pods.yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: mypodquota
spec:
  hard:
    pods: "10"

$ kubectl create namespace my-app

$ kubectl create -f resource-quota-pods.yaml --namespace=my-app

$ kubectl describe resourcequota mypodquota --namespace=mu-app

We can set a number of Quotas on a per-namespace basis, including but not limited to Pods, Secrets, and ConfigMaps

https://kubernetes.io/docs/tasks/administer-cluster/quota-api-object/
https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/


Labeling an Object:
$ kubectl label pods mypods tier=frontend

$ kubectl label --help
kubectl label command can be used to REMOVE labels, OVERWRITE existing laels, and even LABEL ALL RESOURCES in a namespace

Using Labels for Queries:
$ kubectl get pods --show-labels

$ kubectl get pods --selector app=myapp

-l is the short form of --selector

$ kubectl get pods -l app=myapp

Labels are used by Kubernetes for Pod selection by Deployments and Services

We can also define labels in an object manifest like below...

apiVersion: v1
kind: Pod
metadata:
  name: foobar
  labels:
    tire: frontend
    env: development
    app: ui

Set-Based querying:
Many Object kinds support query in the form of "must be labelled with X and/or Y"
Example:
kubectl get pods -l 'env in (production, development)'
This would give Pods that are either in the production or development environment

-L will add a column to the results returned with the value of the specified label.

$ kubectl get pods -Lapp
NAME                   READY   ...   APP
barfoo-7681199-h3gwx   1/1     ...   barfoo
foobar-1123601-6x9w1   1/1     ...   foobar

https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/


Annotating a Resource with kubectl:

$ kubectl annotate pods foobar description='something that we can use for automation"

$ kubectl annotate deployment foobar kubernetes.io/change-cause="Reason for creating a new version"



Managing Specialized Workloads:

Running a Batch Job
Job resource in Kubernetes is used to launch and supervise Pods that will carry out the batch process.
Job resource launches Pods in Run-To-Completion, i.e. Pods terminate after they are done with the task

$ cat counter-batch-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: counter
spec:
  template:
    metadata:
      name: counter
    spec:
      containers:
      - name: counter
        image: busybox
        command:
         - "sh"
         - "-c"
         - "for i in 1 2 3 ; do echo $i ; done"
       restartPolicy: Never

$ kubectl create -f counter-batch-job.yaml
job "counter" created

$ kubectl get jobs
NAME    DESIRED    SUCCESSFUL   AGE
counter 1          1            22s

$ kubectl describe jobs/counter

$ kubectl logs jobs/counter

$ kubectl delete jobs/counter

https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/


Running a Task on a Schedule:

CronJob object is a derivative of the more generic Job object.
schedule section of the spec follows the crontab format.
template section describes the Pods that will run and the command that will get executed.

$ cat cron-job.yaml
apiVersion: batch/v2alpha1
kind: CronJob
metadata:
  name: hourly-date
spec:
  schedule: "0 * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: date
            image: busybox
            command:
              - "sh"
              - "-c"
              - "date"
          restartPolicy: OnFailure

$ kubectl create -f cron-job.yaml

https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/


DaemonSets:
Daemonsets are usually used to launch agents on all the nodes in a cluster.

$ cat fluentd-daemonset.yaml
apiVersion: extensions/v1beta1
kind: DaemonSer
metadata:
  name: fluentd
spec:
  template:
    metadata:
      name: fluentd
      labels:
        app: fluentd
    spec:
      containers:
      - name: fluentd
        image: gcr.io/google_containers/fluentd-elasticsearch:1.3
        env:
         - name: FLUENTD_ARGS
           value: -qq
        volumeMounts:
         - name: varlog
           mountPath: /varlog
         - name: containers
           mountPath: /var/lib/docker/containers
      volumes:
         - hostPath:
             path: /var/log
           name: varlog
         - hostPath:
             path: /var/lib/docker/containers
           name: containers

$ kubectl create -f fluentd-daemonset.yaml

$ kubectl get ds

$ kubectl describe ds/fluentd


Managing Stateful and Leader/Follower Apps:

StatefulSet enables workloads with unique network names, graceful deployment/scaling/termination, and persistent storage.
Below listed CockroachDB installation contains StatefilSet at its core.

apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: cockroachdb
spec:
  serviceName: "cockroachdb"
  replicas: 3
  template:
    metadata:
      labels:
        app: cockroachdb
    spec:
      initContainers:
      - name: bootstrap
        image: cockroachdb/cockroach-k8s-init:0.2
        imagePullPolicy: ifNotPresent
        args:
        - "-on-start=/on-start.sh"
        - "-service=cockroachdb"
        env:
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        volumeMounts:
        - name: datadir
          mountPath: "/cockroach/cockroach-data"
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - cockroachdb
              topologyKey: kubernetes.io/hostname
      containers:
      - name: cockroachdb
        image: cockroachdb/cockroach:v1.0.3
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 26257
          name: grpc
        - containerPort: 8080
          name: http
        volumeMounts:
        - name: datadir
          mountPath: /cockroach/cockroach-data
        command:
          - "/bin/bash"
          - "-ecx"
          - |
            if [ ! "$(hostname)" == "cockroachdb-0" ] || [ -e "/cockroach/cockroach-data/cluster_exists_marker" ]
            then
              CRARGS+=("--join" "cockroachdb-public")
            fi
            exec /cockroach/cockroach ${CRARGS[*]}
      terminationGracePeriodSeconds: 60
      volumes:
      - name:  datadir
        persistentVolumeClaim:
          claimName: datadir
  volumeClaimtemplates:
  - metadata:
      name: datadir
      annotations:
        volume.alpha.kubernetes.io/storage-class: anything
    spec:
      accessModes:
        - "ReadWriteOnce"
      resources:
        requests:
          storage: 1Gi

https://github.com/kubernetes/examples/tree/master/staging/cockroachdb

https://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/

https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/


Pod Startup Behavior (Init Containers):

Sometimes our Pod depends on some other service being available in order to function properly.

e.g. we want to launch an nginx web server that depends on a backend service to serve content.
we have to make sure that the nginx pod only starts up once the backend service is up and running.

$ kubectl run backend --image=mhausenblas/simpleservice:0.5.0
deployment "backend" created

$ kubectl expose deployment backend --port=80 --target-port=9876

Below manifest makes sure that it starts up nginx only when the backend deployment serves data:

$ cat nginx-init-container.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: webserver
        image: nginx
        ports:
        - containerPort: 80
      initContainers:
      - name: checkbackend
        image: busybox
        command: ['sh', '-c', 'until nslookup backend.default.svc; do echo "waiting for backend to be ready"; sleep 3; done;
                   echo "Backend is up, ready to launch web server"']

$ kubectl create -f nginx-init-container.yaml
deployment "nginx" created

$ kubectl get po

$ kubectl logs nginx-2101406530-jwghn -c checkbackend


Volumes and Configuration Data:

Volume is a directory accessible to all containers running in a Pod.
Data in the Volume is preserved across restarts of individual containers in the Pod

> Node-Local Volumes such as emptyDir and hostPath
> Generic networked volumes, such as nfs, glusterfs, or cephfs
> Cloud provider specific volumes, such as awsElasticBlockStore, azureDisk, or gcePersistentDisk
> Special-purpose volumes, such as ConfigMaps, Secrets, or gitRepo

$ cat emptydir-volume-example.yaml
apiVersion: v1
kind: Pod
metadata:
  name: sharevol
spec:
  containers:
  - name: c1
    image: centos:7
    command:
      - "bin/bash"
      - "-c"
      - "sleep 10000"
    volumeMounts:
      - name: xchange
        mountPath: "/tmp/xchange"
  - name: c2
    image: centos:7
    command:
      - "bin/bash"
      - "-c"
      - "sleep 10000"
    volumeMounts:
      - name: xchange
        mountPath: "/tmp/data"
  volumes:
  - name: xchange
    emptyDir: {}

$ kubectl create -f emptydir-volume-example.yaml
pod "sharevol" created

$ kubectl exec sharevol -c c1 -i -t -- bash
# mount | grep xchange
/dev/vdal on /tmp/xchange type ext4 (rw,relatime,data=ordered)
# echo 'some data' > /tmp/xchanging/data
# exit

$ kubectl exec sharevol -c c2 -i -t -- bash
# mount | grep /tmp/data
/dev/vdal on /tmp/data type ext4 (rw,relatime,data=ordered)
# cat /tmp/data/data
some data

Local Volume is backed by the Node where the Pod is running.
We will loose the Local Volume and all the data if the Node goes down.

https://kubernetes.io/docs/concepts/storage/volumes/

Secret as Volume:

$ echo -n "open sesame" > ./passphrase

$ kubectl create secret generic pp --from-file=./passphrase
secret "pp" created

$ kubectl describe secrets/pp

$ cat pod-secret-as-volume.yaml
apiVersion: v1
kind: Pod
metadata:
  name: ppconsumer
spec:
  containers:
  - name: shell
    image: busybox
    command:
      - "sh"
      - "-c"
      - "mount | grep access && sleep 3600"
    volumeMounts:
      - name: passphrase
        mountPath: "/tmp/access"
        readOnly: true
  volumes:
  - name: passphrase
    secret:
      secretName: pp

$ kubectl create -f pod-secret-as-volume.yaml
pod "ppconsumer" created

$ kubectl log ppconsumer

$ kubectl exec ppconsumer -i -t -- sh
# cat /tmp/access/passphrase
open sesame

> Secrets exist in the context of a namespace, se we need to take that in to account when setting up and consuming secrets.
> Secrets can be accessed either as Volumes or Environment Variables from within a Pod
> Size of a Secret is limited to 1 MB

"kubectl create seret" deals with three types of secrets
1. "generic" type secret: creates a secret from a local file, directory or literal value
2. "docker-registry" type secret: for use with a Docker registry
3. "tls" type secret: To create SSL certificates

"kubectl describe" doesn't show the content of a Secret in plain text.

Decoding a Secret
$ kubectl get secret pp -o yaml | grep passphrase | cut -d":" -f 2 | aws '{$1=$1};1' | base64 --decode

> "grep" pulls out the line "passphrase: b3BlbiBzZXNhbWU="
> "cut" extracts the content of the passphrase
> "awk" gets rid of the leading whitespace
> "base64" turns it in to the original data

NOTE:
We can use the --experimental-encryption-provider-config option when launching kube-apiserver
This is supported after v1.7, and this will encrypt the base64 encoded secret before storing it in etcd

https://kubernetes.io/docs/concepts/configuration/secret/
https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/


ConfigMaps : Providing Configuration Data to an Application:

$ kubectl create configmap siseconfig --from-literal=siseversion=0.9
configmap "siseconfig" created

$ cat comfigmap-app.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: configmapapp
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: configmapapp
    spec:
      containers:
      - name: sise
        image: mhausenblas/simpleservice:0.5.0
        ports:
        - containerPort: 9876
        env:
        - name: SIMPLE_SERVICE_VERSION
          valueFrom:
            configMapKeyRef:
              name: siseconfig
              key: siseversion

NOTE:
Environment variables are populated at the Pod startup time.
Any changes to the ConfigMap are not reflected in the Pod's Environment Variables

$ cat multi-key-value-cm.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: special-config
  namespace: default
data:
  SPECIAL_LEVEL: very
  SPECIAL_TYPE: charming

$ kubectl create -f multi-key-value-cm.yaml

$ cat multi-key-value-cm-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: dapi-test-pod
spec:
  containers:
  - name: test-container
    image: k8s.gcr.io/busybox
    command: ["/bin/sh", "-c", "env"]
    envFrom:
    - configMapRef:
        name: special-config
  restartPolicy: Never

$ kubectl create -f multi-key-value-cm-pod.yaml

https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/

$ cat example.cfg
debug: true
home: ~/abc

$ kubectl create configmap configexample --from-file=example.cfg

$ cat configfile-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: oreilly
spec:
  containers:
  - image: busybox
    command:
      - sleep
      - "3600"
    volumeMounts:
    - mountPath: /oreilly
      name: orielly
    name: busybox
  volumes:
  - name: orielly
    configMap:
      name: configexample

$ kubectl create -f configfile-pod.yaml

$ kubectl exec -it oreilly -- ls -l /oreilly
total 0
lrwxrwxrwx 1 root  root  18 Oct 18 19:36 example.cfg -> ..data/example.cfg

$ kubectl exec -ti oreilly -- cat /oreilly/example.cfg
debug: true
home: ~/abc


Persistent Volume with Minikube:

$ hostpath-pv.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: hostpathpv
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 1Gi
  accessModes:
  - ReadWriteOnce
  hostPath:
    path: "/tmp/pvdata"

$ minikube ssh

$ mkdir /tmp/pvdata && echo 'This content is served from the Persistent Volume' > /tmp/pvdata/index.html

$ cat /tmp/pvdata/index.html
This content is served from the Persistent Volume

$ exit

$ kubectl create -f hostpath-pv.yaml
persistentvolume "hostpathpv" created

$ kubectl get pv/hostpathpv
Here we should see the STATUS as Available

$ kubectl describe pv/hostpathpv

$ cat hostpath-pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mypvc
spec:
  storageClassName: manual
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 200Mi

$ kubectl create -f hostpath-pvc.yaml
persistentvolumeclaim "mypvc" created

$ kubectl get pv/hostpathpv
Here we should see the STATUS as Bound

$ cat pod-with-pv.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: pod-with-pv
spec:
  replicas:
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: webserver
        image: nginx
        ports:
        - containerPort: 80
        volumeMounts:
        - mountPath: "/usr/share/nginx/html"
          name: webservercontent
      volumes:
      - name:
        persistentVolumeClaim:
          claimName: mypvc

$ kubectl create -f pod-with-pv.yaml

$ kubectl get pvc/mypvc
Here we shoudl see STATUS as Bound

$ curl -k -s https://192.168.99.100/web
This content is served from the Persistent Volume

PersistentVolumes are cluster-wide resources
PersistentVolumeClaims are namespaced resources.

https://kubernetes.io/docs/concepts/storage/persistent-volumes/

https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/


Data Persistency on Minikube:

$ cat data.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: data
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi

On Minikube create this PVC and immediately see how a Persistent Volume is created to match this claim:

$ kubectl create -f data.yaml

$ kubectl get pvc
We should see STATUS as Bound here

$ kubectl get pv
NAME                                      CAPACITY  ... ...
pvc-da58c85c-e29a-11e7-ac0b-080027fcc0e7  1Gi       ... ...

NOTE:
Minikube is configured out of the box with a default storage class that defines a default Persistent Volume provisioner.
i.e. when a Persistent Volume Claim is created, Kubernetes will dynamically create a matching Persistent Volume to fill that claim

$ kubectl get storageclass
NAME                PROVISIONER
standard (default)  k8s.io/minikube-hostpath

$ kubectl get storageclass standard -o yaml
...

$ kubectl get pv pvc-da58c85c-e29a-11e7-ac0b-080027fcc0e7 -o yaml


$ cat mysql-pv.yaml
apiVersion: v1
kind: Pod
metadata:
  name: db
spec:
  containers:
  - image: mysql:5.5
    name: db
    volumeMounts:
    - mountPath: /var/lib/mysql
      name: data
    env:
      - name: MYSQL_ROOT_PASSWORD
        value: root
  volumes:
  - name: data
    persistentVolumeClaim:
      claimName: data

$ kubectl create -f mysql-pv.yaml

https://kubernetes.io/docs/concepts/storage/persistent-volumes/

https://kubernetes.io/docs/concepts/storage/storage-classes/


Dynamically Provisioning Persistent Storage on GKE

$ cat slow-storage.yaml
apiVersion: storage.k8s.io/v1beta1
kind: StorageClass
metadata:
  name: slow
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-standard

$ kubectl create -f slow-storage.yaml
This will create a StorageClass named "slow" which will provision standard disk like Persistent Disks

$ cat fast-storage.yaml
apiVersion: storage.k8s.io/v1beta1
kind: StorageClass
metadata:
  name: fast
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-ssd

$ kubectl create -f slow-storage.yaml
This will create a StorageClass named "fast" which will provision SSD like Persistent Disks

Users request dynamically provissioned storage by including a Storage Class in their PersistentVolumeClaim.
In beta version, Storage Class is specified with volume.beta.kubernetes.io/stoarge-class annotation

$ cat fast-pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: claim1
  annotations:
    volume.beta.kubernetes.io/storage-class: fast
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 30Gi



Scaling:
> Cluster Scaling
> Horizontal Pod Autoscalers
> Vertical Pod Autoscalers

Manual Scaling with "kubectl scale" Command:

$ kubectl get deployment myfancyapp
NAME         DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
myfancyapp   5         5         5            5           9m

$ kubectl scale deployment myfancyapp --replicas=3
deployment "myfancyapp" scaled

$ kubectl get deployment myfancyapp
NAME         DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
myfancyapp   3         3         3            3           9m


Automatically Resizing a Cluster in GKE:
$ gcloud container clusters create --num-nodes=1 supersizeme

$ gcloud container clusters get-credentials supersizeme --zone europe-west2-b --project k8s-cookbook

Enabling Cluster Autoscaling:
$ gcloud beta container clusters update supersizeme --enable-autoscaling --min-nodes=1 --max-nodes=3 \
 --zone europe-west2-b --project k8s-cookbook

Test Cluster Autoscaling:
$ kubectl run ghost --image=ghost:0.9 --replicas=15

https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler


Horizontal Pod Autoscaler:
$ kubectl run appserver --image=gcr.io/google_containers/hpa-example --requests=cpu=200m --expose --port=80
service "appserver" created
deployment "appserver" created

$ kubectl autoscale deployment appserver --cpu-percent=40 --min=1 --max=5
deployment "appserver" autoscaled

$ kubectl get hpa -watch

In a second terminal session keep watch on the deployment
$ kubectl get deployment appserver --watch

In a third terminal session, launch the load generator
$ kubectl run -i -t loadgen --image=busybox /bin/sh
# while true; do wget -q -O- http://appserver.default.svc.cluster.local; done

https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/

https://kubernetes.io/blog/2016/07/autoscaling-in-kubernetes/

https://github.com/mhausenblas/k8s-autoscale
https://github.com/mhausenblas/container-networking


Security:

Providing a Unique Identity for an Application:

$ kubectl create serviceaccount myappsa
serviceaccount "myappsa" created

$ kubectl describe sa mypodsa
Name: myappsa
Namespace: default
Labels: <none>
Annotations: <none>

Image pull secrets: <none>

Mountable secrets: myappsa-token-rr6jc

Tokens: myappsa-token-rr6jc

$ kubectl describe secret myappsa-token-rr6jc

$ cat mypod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: myapp
spec:
  serviceAccountName: myappsa
  containers:
  - name: main
    image: centos:7
    command:
      - "bin/bash"
      - "-c"
      - "sleep 10000"

$ kubectl create -f mypod.yaml

$ kubectl exec myapp -c main cat /var/run/secrets/kubernetes.io/serviceaccount/token
eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9..

Serviceaccounts are namespaced resources

$ kubectl get sa
NAME         SECRETS   AGE
default      1         90d
myapp        1         19m
prometheus   1         89d

Notice the "default" service account.
This is created automatically and will be assigned to Pods without any explicit service accounts

https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/

https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/

https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/

https://kubernetes.io/docs/reference/access-authn-authz/authorization/


Listing and Viewing Access Control Information:

We can check if a certain action on a resource is allowed for a specific user.
This is only possible when we use Role-Based Access Control as the authorization method

$ kubectl auth can-i list pods --as=system:serviceaccount:sec:myappsa -n=sec
Command to check if the "myappsa" Service Account has access to list Pods in "sec" namespace

$ kubectl get roles -n=kube-system
Command to list roles available in "kube-system" namespace

$ kubectl get clusterroles -n=kube-system
Command to list Cluster Roles available in "kube-system" namespace

$ kubectl describe clusterroles/view -n=kube-system
Describe the "view" Cluster Role in "kube-system" namespace

RBAC Authorization:
> An entity - that is, a group, user, or service account
> A resource, such as Pod, Service, or Secret
> A role, which defines rules for actions on a resource
> A role binding, which applies a role to an entity

Actions on a resource that a role uses in its rules (verbs):
> get, list, watch
> create
> update/patch
> delete

Two types of Roles:
> Cluster-Wide: Cluster Roles and their respective Cluster Role Bindings
> Namespace-Wide: Roles and Role Binding

https://kubernetes.io/docs/reference/access-authn-authz/rbac/


Controlling Access to Resources:

Let's assume that we want to restrict an app to only be able to view Pods.
i.e. it can only list Pods, and get details about Pods

$ kubectl create serviceaccount myappsa
serviceaccount "myappsa" created

$ cat pod-with-sa.yaml
apiVersion: v1
kind: Pod
metadata:
  name: myapp
  namespace: sec
spec:
  serviceAccountName: myappsa
  containers:
  - name: main
    image: centos:7
    command:
      - "bin/bash"
      - "-c"
      - "sleep 10000"

Now we can define a Role "podreader" to give access only to read Pods

$ cat pod-reader.yaml
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: podreader
  namespace: sec
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list"]

Now that we have the podreader Role, we have to apply it to Secrvice Account myappsa using a RoleBinding

$ cat pod-reader-binding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: podreaderbinding
  namespace: sec
roleRef:
  apiGroup: rbac.authorization.k8s.io/v1
  kind: Role
  name: podreader
subjects:
- kind: ServiceAccount
  name: mypodsa
  namespace: sec

$ kubectl create -f pod-reader.yaml

$ kubectl create -f pod-reader-binding.yaml

$ kubectl create -f pod-with-sa.yaml


We can also create Role and RoleBinding with kubectl as below.

$ kubectl create role podreader --verb=get --verb=list --resource=pods -n=sec

$ kubectl create rolebinding podreaderbinding --role=sec:podreader --serviceaccount=sec:myappsa --namespace=sec -n=sec

NOTE:
For cluster-wide access control, we should use ClusterRole and ClusterRoleBinding

=> If you want to restrict access to a namespaced resource (like a Service or Pod) in a certain namespace, use Role and RoleBinding

=> If you want to reuse a Role in a couple of namespaces, use a ClusterRole with a RoleBinding

=> If you want to restrict access to cluster-wide resources such as nodes or to namespaced resources across all namespaces, use ClusterRole with a ClusterRoleBinding


Securing Pods (Security Context):
We might want an app to run as nonprivileged process or restrict the types of Volumes an app can access.

$ cat secure-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: secpod
spec:
  containers:
  - name: shell
    image: centos:7
    command:
      - "bin/bash"
      - "-c"
      - "sleep 10000"
    securityContext:
      runAsUser: 5000

$ kubectl create -f secure-pod.yaml
pod "secpod" created

$ kubectl exec secpod ps aux
USER PID %CPU %MEM   VSZ    RSS  TTY  STAT  START  TIME  COMMAND
5000 1   0.0  0.0    4328   672  ?    Ss    12:39  0:00  sleep 10000
5000 8   0.0  0.1   47460  3108  ?    Rs    12:40  0:00  ps aux

NOTE:
A more poweful method to enforce policies on Pod level is to use POD SECURITY POLICY
POD SECURITY POLICIES are cluster-wide resources that allow us to define a range of policies,
including some similar to when we have seen in the securityContext example above.

securityContext: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/

PodSecurityPolicy: https://kubernetes.io/docs/concepts/policy/pod-security-policy/


Pod Logs:

$ kubectl logs --help | more

$ kubectl get pods
NAME                     ...   ...
ghost-8449997474-kn86m   ...   ...

$ kubectl logs -f ghost-8449997474-kn86m
...
...

NOTE: If a Pod has multiple containers, we can get the logs of a specific container by specifying -c option

$ kubectl logs <POD-NAME> -c <CONTAINER-NAME>


Liveness Probe:
If the application running as Pod gets in to a broken state, we want Kubernetes to restart the Pod automatically

With "Liveness Probe", kubelet restarts the Pod automatically if the probe fails.
Liveness Probe is part of the Pod specification added to the containers section.
Each container in a Pod can have a Liveness Probe defined

Three types of probe:
1. Command that is executed inside the container
2. HTTP request to a specific route served by a webserver inside the container
3. More generic TCP probe

$ cat webserver.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webserver
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:stable
        ports:
        - containerPort: 80
        livenessProbe:
          initialDelaySecond: 2
          periodSeconds: 10
          httpGet:
            path: /
            port: 80
        readinessProbe:
          initialDelaySecond: 2
          periodSeconds: 10
          httpGet:
            path: /
            port: 80

https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/
https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/



